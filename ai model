import os
import numpy as np
import tensorflow as tf
from tensorflow.keras.applications import MobileNetV2, EfficientNetB0
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Dropout, GlobalAveragePooling2D, Concatenate
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import BinaryCrossentropy
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler
from tensorflow.keras.preprocessing.image import ImageDataGenerator

tf.keras.mixed_precision.set_global_policy('mixed_float16')

train_dir = '/content/clahe_dataset/train'
test_dir = '/content/clahe_dataset/test'

def get_train_datagen():
    return ImageDataGenerator(
        rescale=1./255,
        rotation_range=20,
        width_shift_range=0.1,
        height_shift_range=0.1,
        shear_range=0.15,
        zoom_range=0.15,
        horizontal_flip=True,
        brightness_range=[0.8, 1.2],
        fill_mode='nearest',
        validation_split=0.15
    )

def get_val_datagen():
    return ImageDataGenerator(
        rescale=1./255,
        validation_split=0.15
    )

train_datagen = get_train_datagen()
val_datagen = get_val_datagen()

img_size = (224, 224)
batch_size = 32

train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=img_size,
    batch_size=batch_size,
    class_mode='binary',
    subset='training',
    shuffle=True,
    seed=42
)

val_generator = val_datagen.flow_from_directory(
    train_dir,
    target_size=img_size,
    batch_size=batch_size,
    class_mode='binary',
    subset='validation',
    shuffle=False,
    seed=42
)

test_datagen = ImageDataGenerator(rescale=1./255)
test_generator = test_datagen.flow_from_directory(
    test_dir,
    target_size=img_size,
    batch_size=batch_size,
    class_mode='binary',
    shuffle=False
)

# MixUp helper function
def mixup(batch_x, batch_y, alpha=0.2):
    lam = np.random.beta(alpha, alpha)
    batch_size = batch_x.shape[0]
    index = np.random.permutation(batch_size)
    mixed_x = lam * batch_x + (1 - lam) * batch_x[index]
    mixed_y = lam * batch_y + (1 - lam) * batch_y[index]
    return mixed_x, mixed_y

def mixup_generator(generator):
    while True:
        batch_x, batch_y = next(generator)
        batch_x, batch_y = mixup(batch_x, batch_y)
        yield batch_x, batch_y

train_mixup_generator = mixup_generator(train_generator)

input_tensor = Input(shape=(224, 224, 3))

mobilenet_base = MobileNetV2(include_top=False, weights='imagenet', input_tensor=input_tensor)
efficientnet_base = EfficientNetB0(include_top=False, weights='imagenet', input_tensor=input_tensor)

for layer in mobilenet_base.layers:
    layer.trainable = False
for layer in efficientnet_base.layers:
    layer.trainable = False

mobilenet_out = GlobalAveragePooling2D()(mobilenet_base.output)
efficientnet_out = GlobalAveragePooling2D()(efficientnet_base.output)

combined = Concatenate()([mobilenet_out, efficientnet_out])
x = Dense(512, activation='relu')(combined)
x = Dropout(0.5)(x)
x = Dense(128, activation='relu')(x)
x = Dropout(0.5)(x)
output = Dense(1, activation='sigmoid', dtype='float32')(x)

model = Model(inputs=input_tensor, outputs=output)

loss_fn = BinaryCrossentropy(label_smoothing=0.1)
optimizer = Adam(learning_rate=1e-4)

model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])

def scheduler(epoch, lr):
    if epoch < 5:
        return lr
    else:
        return lr * tf.math.cos((epoch - 5) * 3.1415 / 20).numpy()

callbacks = [
    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),
    ModelCheckpoint('/content/drive/My Drive/pneumonia_best_model_clahe5.keras', save_best_only=True, monitor='val_accuracy', mode='max'),
    ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=3, verbose=1),
    LearningRateScheduler(scheduler)
]

# Phase 1: Train only top layers with MixUp augmentation
history_phase1 = model.fit(
    train_mixup_generator,
    steps_per_epoch=train_generator.samples // batch_size,
    validation_data=val_generator,
    validation_steps=val_generator.samples // batch_size,
    epochs=10,
    callbacks=callbacks
)

# Phase 2: Unfreeze last 50 layers for fine-tuning with lower LR and less label smoothing
for layer in mobilenet_base.layers[-50:]:
    layer.trainable = True
for layer in efficientnet_base.layers[-50:]:
    layer.trainable = True

model.compile(
    optimizer=Adam(learning_rate=1e-5),
    loss=BinaryCrossentropy(label_smoothing=0.05),
    metrics=['accuracy']
)

history_phase2 = model.fit(
    train_mixup_generator,
    steps_per_epoch=train_generator.samples // batch_size,
    validation_data=val_generator,
    validation_steps=val_generator.samples // batch_size,
    epochs=10,
    callbacks=callbacks
)

loss, accuracy = model.evaluate(test_generator)
print(f'Test accuracy on unseen data: {accuracy:.4f}')

model.save('/content/drive/My Drive/final_pneumonia_model_clahe_mixed.h5')
